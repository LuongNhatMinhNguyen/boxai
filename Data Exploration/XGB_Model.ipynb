{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d9eeea57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup: shared imports, constants, metrics, helpers\n",
    "import os\n",
    "import re\n",
    "import math\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import duckdb as ddb\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "RANDOM_STATE = 42\n",
    "EPS = 1.0\n",
    "DB_PATH = '../data/numero.duckdb'\n",
    "\n",
    "\n",
    "def wape(y_true, y_pred):\n",
    "    denom = np.sum(np.abs(y_true)) + 1e-9\n",
    "    return float(np.sum(np.abs(y_true - y_pred)) / denom) if denom > 0 else np.nan\n",
    "\n",
    "def smape(y_true, y_pred):\n",
    "    denom = (np.abs(y_true) + np.abs(y_pred))\n",
    "    return float(np.mean(2.0 * np.abs(y_pred - y_true) / np.maximum(denom, 1e-9)))\n",
    "\n",
    "def mape_eps(y_true, y_pred, eps=EPS):\n",
    "    return float(np.mean(np.abs((y_true - y_pred) / np.maximum(np.abs(y_true), eps))))\n",
    "\n",
    "def mae(y_true, y_pred):\n",
    "    return float(np.mean(np.abs(y_true - y_pred)))\n",
    "\n",
    "def rmse(y_true, y_pred):\n",
    "    return float(np.sqrt(np.mean((y_true - y_pred) ** 2)))\n",
    "\n",
    "def r2(y_true, y_pred):\n",
    "    return float(r2_score(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7979e2e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 1 (XGB, test) metrics:\n",
      "- MAPE_eps1: 0.3229\n",
      "- R2: 0.8204\n",
      "- WAPE: 0.3191\n",
      "- SMAPE: 0.3138\n",
      "- MAE: 47109513.7283\n",
      "- RMSE: 199400512.8425\n"
     ]
    }
   ],
   "source": [
    "# Model 1: XGBoost (log-space) — simple metrics (no rebuild/clean)\n",
    "import xgboost as xgb\n",
    "\n",
    "con = ddb.connect(DB_PATH)\n",
    "df = con.execute('SELECT wk1_total, final_total FROM features_afterN1_total').df()\n",
    "\n",
    "# Split\n",
    "train_df, test_df = train_test_split(df, test_size=0.2, random_state=RANDOM_STATE)\n",
    "\n",
    "# Log features\n",
    "Xtr = np.log1p(train_df[['wk1_total']].values.astype('float64'))\n",
    "Ytr = np.log1p(train_df['final_total'].values.astype('float64'))\n",
    "Xte = np.log1p(test_df[['wk1_total']].values.astype('float64'))\n",
    "Yte = test_df['final_total'].values.astype('float64')\n",
    "\n",
    "# Validation split for early stopping\n",
    "from sklearn.model_selection import train_test_split as tts\n",
    "Xtr_sub, Xval, Ytr_sub, Yval = tts(Xtr, Ytr, test_size=0.2, random_state=RANDOM_STATE)\n",
    "\n",
    "# DMatrix\n",
    "dtrain = xgb.DMatrix(Xtr_sub, label=Ytr_sub)\n",
    "dval = xgb.DMatrix(Xval, label=Yval)\n",
    "dtest = xgb.DMatrix(Xte)\n",
    "\n",
    "params = {\n",
    "    'objective': 'reg:squarederror',\n",
    "    'eval_metric': 'rmse',\n",
    "    'max_depth': 3,\n",
    "    'eta': 0.05,\n",
    "    'subsample': 0.9,\n",
    "    'colsample_bytree': 1.0,\n",
    "    'lambda': 1.0,\n",
    "    'tree_method': 'hist',\n",
    "    'seed': RANDOM_STATE,\n",
    "}\n",
    "\n",
    "bst = xgb.train(\n",
    "    params=params,\n",
    "    dtrain=dtrain,\n",
    "    num_boost_round=800,\n",
    "    evals=[(dval, 'validation')],\n",
    "    early_stopping_rounds=50,\n",
    "    verbose_eval=False\n",
    ")\n",
    "\n",
    "# Predict in original space\n",
    "if hasattr(bst, 'best_ntree_limit') and bst.best_ntree_limit:\n",
    "    pred_te = np.expm1(bst.predict(dtest, ntree_limit=bst.best_ntree_limit))\n",
    "elif hasattr(bst, 'best_iteration') and bst.best_iteration is not None:\n",
    "    pred_te = np.expm1(bst.predict(dtest, iteration_range=(0, bst.best_iteration + 1)))\n",
    "else:\n",
    "    pred_te = np.expm1(bst.predict(dtest))\n",
    "\n",
    "# Metrics (test)\n",
    "metrics = {\n",
    "    'MAPE_eps1': mape_eps(Yte, pred_te, eps=1.0),\n",
    "    'R2': r2(Yte, pred_te),\n",
    "    'WAPE': wape(Yte, pred_te),\n",
    "    'SMAPE': smape(Yte, pred_te),\n",
    "    'MAE': mae(Yte, pred_te),\n",
    "    'RMSE': rmse(Yte, pred_te)\n",
    "}\n",
    "\n",
    "print('Model 1 (XGB, test) metrics:')\n",
    "for k_, v_ in metrics.items():\n",
    "    print(f\"- {k_}: {v_:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a6bca9de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 2 (XGB, test) metrics:\n",
      "- MAPE_eps1: 5.1318\n",
      "- R2: 0.9507\n",
      "- WAPE: 0.2158\n",
      "- SMAPE: 0.5288\n",
      "- MAE: 9204942.9101\n",
      "- RMSE: 27966978.0526\n"
     ]
    }
   ],
   "source": [
    "# Model 2: XGBoost (log-space) for week-2 weekly — simple metrics (no rebuild/clean)\n",
    "import xgboost as xgb\n",
    "\n",
    "con = ddb.connect(DB_PATH)\n",
    "df2 = con.execute('SELECT wk1_total, wk2_weekly FROM features_afterN1_week2').df()\n",
    "\n",
    "# Split\n",
    "train_df2, test_df2 = train_test_split(df2, test_size=0.2, random_state=RANDOM_STATE)\n",
    "\n",
    "# Log features\n",
    "Xtr2 = np.log1p(train_df2[['wk1_total']].values.astype('float64'))\n",
    "Ytr2 = np.log1p(train_df2['wk2_weekly'].values.astype('float64'))\n",
    "Xte2 = np.log1p(test_df2[['wk1_total']].values.astype('float64'))\n",
    "Yte2 = test_df2['wk2_weekly'].values.astype('float64')\n",
    "\n",
    "# Validation split\n",
    "from sklearn.model_selection import train_test_split as tts\n",
    "Xtr2_sub, Xval2, Ytr2_sub, Yval2 = tts(Xtr2, Ytr2, test_size=0.2, random_state=RANDOM_STATE)\n",
    "\n",
    "# DMatrix\n",
    "dtrain2 = xgb.DMatrix(Xtr2_sub, label=Ytr2_sub)\n",
    "dval2 = xgb.DMatrix(Xval2, label=Yval2)\n",
    "dtest2 = xgb.DMatrix(Xte2)\n",
    "\n",
    "params2 = {\n",
    "    'objective': 'reg:squarederror',\n",
    "    'eval_metric': 'rmse',\n",
    "    'max_depth': 3,\n",
    "    'eta': 0.05,\n",
    "    'subsample': 0.9,\n",
    "    'colsample_bytree': 1.0,\n",
    "    'lambda': 1.0,\n",
    "    'tree_method': 'hist',\n",
    "    'seed': RANDOM_STATE,\n",
    "}\n",
    "\n",
    "bst2 = xgb.train(\n",
    "    params=params2,\n",
    "    dtrain=dtrain2,\n",
    "    num_boost_round=800,\n",
    "    evals=[(dval2, 'validation')],\n",
    "    early_stopping_rounds=50,\n",
    "    verbose_eval=False\n",
    ")\n",
    "\n",
    "# Predict\n",
    "if hasattr(bst2, 'best_ntree_limit') and bst2.best_ntree_limit:\n",
    "    pred_te2 = np.expm1(bst2.predict(dtest2, ntree_limit=bst2.best_ntree_limit))\n",
    "elif hasattr(bst2, 'best_iteration') and bst2.best_iteration is not None:\n",
    "    pred_te2 = np.expm1(bst2.predict(dtest2, iteration_range=(0, bst2.best_iteration + 1)))\n",
    "else:\n",
    "    pred_te2 = np.expm1(bst2.predict(dtest2))\n",
    "\n",
    "# Metrics (test)\n",
    "metrics2 = {\n",
    "    'MAPE_eps1': mape_eps(Yte2, pred_te2, eps=1.0),\n",
    "    'R2': r2(Yte2, pred_te2),\n",
    "    'WAPE': wape(Yte2, pred_te2),\n",
    "    'SMAPE': smape(Yte2, pred_te2),\n",
    "    'MAE': mae(Yte2, pred_te2),\n",
    "    'RMSE': rmse(Yte2, pred_te2)\n",
    "}\n",
    "\n",
    "print('Model 2 (XGB, test) metrics:')\n",
    "for k_, v_ in metrics2.items():\n",
    "    print(f\"- {k_}: {v_:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
