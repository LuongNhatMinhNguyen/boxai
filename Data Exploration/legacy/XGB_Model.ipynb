{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d9eeea57",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import math\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import duckdb as ddb\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "RANDOM_STATE = 42\n",
    "EPS = 1.0\n",
    "DB_PATH = '../data/numero.duckdb'\n",
    "\n",
    "\n",
    "def wape(y_true, y_pred):\n",
    "    denom = np.sum(np.abs(y_true)) + 1e-9\n",
    "    return float(np.sum(np.abs(y_true - y_pred) / denom)) if denom > 0 else np.nan\n",
    "\n",
    "def smape(y_true, y_pred):\n",
    "    denom = (np.abs(y_true) + np.abs(y_pred))\n",
    "    return float(np.mean(2.0 * np.abs(y_pred - y_true) / np.maximum(denom, 1e-9)))\n",
    "\n",
    "def mape_eps(y_true, y_pred, eps=EPS):\n",
    "    return float(np.mean(np.abs((y_true - y_pred) / np.maximum(np.abs(y_true), eps))))\n",
    "\n",
    "def mae(y_true, y_pred):\n",
    "    return float(np.mean(np.abs(y_true - y_pred)))\n",
    "\n",
    "def rmse(y_true, y_pred):\n",
    "    return float(np.sqrt(np.mean((y_true - y_pred) ** 2)))\n",
    "\n",
    "def r2(y_true, y_pred):\n",
    "    return float(r2_score(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7979e2e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 1 (XGB, test) metrics:\n",
      "- MAPE_eps1: 0.3229\n",
      "- R2: 0.8204\n",
      "- WAPE: 0.3191\n",
      "- SMAPE: 0.3138\n",
      "- MAE: 47109513.7283\n",
      "- RMSE: 199400512.8425\n"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "\n",
    "with ddb.connect(DB_PATH, read_only=True) as con:\n",
    "    df = con.execute('SELECT wk1_total, final_total FROM features_afterN1_total').df()\n",
    "\n",
    "train_df, test_df = train_test_split(df, test_size=0.2, random_state=RANDOM_STATE)\n",
    "\n",
    "Xtr = np.log1p(train_df[['wk1_total']].values.astype('float64'))\n",
    "Ytr = np.log1p(train_df['final_total'].values.astype('float64'))\n",
    "Xte = np.log1p(test_df[['wk1_total']].values.astype('float64'))\n",
    "Yte = test_df['final_total'].values.astype('float64')\n",
    "\n",
    "from sklearn.model_selection import train_test_split as tts\n",
    "Xtr_sub, Xval, Ytr_sub, Yval = tts(Xtr, Ytr, test_size=0.2, random_state=RANDOM_STATE)\n",
    "\n",
    "dtrain = xgb.DMatrix(Xtr_sub, label=Ytr_sub)\n",
    "dval = xgb.DMatrix(Xval, label=Yval)\n",
    "dtest = xgb.DMatrix(Xte)\n",
    "\n",
    "params = {\n",
    "    'objective': 'reg:squarederror',\n",
    "    'eval_metric': 'rmse',\n",
    "    'max_depth': 3,\n",
    "    'eta': 0.05,\n",
    "    'subsample': 0.9,\n",
    "    'colsample_bytree': 1.0,\n",
    "    'lambda': 1.0,\n",
    "    'tree_method': 'hist',\n",
    "    'seed': RANDOM_STATE,\n",
    "}\n",
    "\n",
    "bst = xgb.train(\n",
    "    params=params,\n",
    "    dtrain=dtrain,\n",
    "    num_boost_round=800,\n",
    "    evals=[(dval, 'validation')],\n",
    "    early_stopping_rounds=50,\n",
    "    verbose_eval=False\n",
    ")\n",
    "\n",
    "if hasattr(bst, 'best_ntree_limit') and bst.best_ntree_limit:\n",
    "    pred_te = np.expm1(bst.predict(dtest, ntree_limit=bst.best_ntree_limit))\n",
    "elif hasattr(bst, 'best_iteration') and bst.best_iteration is not None:\n",
    "    pred_te = np.expm1(bst.predict(dtest, iteration_range=(0, bst.best_iteration + 1)))\n",
    "else:\n",
    "    pred_te = np.expm1(bst.predict(dtest))\n",
    "\n",
    "metrics = {\n",
    "    'MAPE_eps1': mape_eps(Yte, pred_te, eps=1.0),\n",
    "    'R2': r2(Yte, pred_te),\n",
    "    'WAPE': wape(Yte, pred_te),\n",
    "    'SMAPE': smape(Yte, pred_te),\n",
    "    'MAE': mae(Yte, pred_te),\n",
    "    'RMSE': rmse(Yte, pred_te)\n",
    "}\n",
    "\n",
    "print('Model 1 (XGB, test) metrics:')\n",
    "for k_, v_ in metrics.items():\n",
    "    print(f\"- {k_}: {v_:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "334fa2b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Artifacts saved to artifacts_final_total\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from boxai.models.final_total_predictor import FinalTotalPredictor\n",
    "\n",
    "# Save trained Model 1 booster (final_total prediction)\n",
    "ART_DIR = \"artifacts_final_total\"\n",
    "os.makedirs(ART_DIR, exist_ok=True)\n",
    "\n",
    "best_ntree_limit = getattr(bst, \"best_ntree_limit\", None)\n",
    "best_iteration = getattr(bst, \"best_iteration\", None)\n",
    "\n",
    "FinalTotalPredictor.save_from_training(\n",
    "    booster=bst,\n",
    "    artifacts_dir=ART_DIR,\n",
    "    best_ntree_limit=best_ntree_limit,\n",
    "    best_iteration=best_iteration,\n",
    "    metrics=metrics,\n",
    "    overwrite=True,\n",
    ")\n",
    "print(f\"Artifacts saved to {ART_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ef6c7244",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nimport xgboost as xgb\\nfrom sklearn.model_selection import train_test_split as tts\\nimport numpy as np\\nimport duckdb as ddb\\n\\nwith ddb.connect(DB_PATH, read_only=True) as con:\\n    df2 = con.execute(\\'SELECT wk1_total, wk2_weekly FROM features_afterN1_week2\\').df()\\n\\ntrain_df2, test_df2 = train_test_split(df2, test_size=0.2, random_state=RANDOM_STATE)\\n\\nXtr2 = np.log1p(train_df2[[\\'wk1_total\\']].values.astype(\\'float64\\'))\\nYtr2 = np.log1p(train_df2[\\'wk2_weekly\\'].values.astype(\\'float64\\'))\\nXte2 = np.log1p(test_df2[[\\'wk1_total\\']].values.astype(\\'float64\\'))\\nYte2 = test_df2[\\'wk2_weekly\\'].values.astype(\\'float64\\')\\n\\nXtr2_sub, Xval2, Ytr2_sub, Yval2 = tts(Xtr2, Ytr2, test_size=0.2, random_state=RANDOM_STATE)\\n\\ndtrain2 = xgb.DMatrix(Xtr2_sub, label=Ytr2_sub)\\ndval2 = xgb.DMatrix(Xval2, label=Yval2)\\ndtest2 = xgb.DMatrix(Xte2)\\n\\nparams2 = {\\n    \\'objective\\': \\'reg:squarederror\\',\\n    \\'eval_metric\\': \\'rmse\\',\\n    \\'max_depth\\': 3,\\n    \\'eta\\': 0.05,\\n    \\'subsample\\': 0.9,\\n    \\'colsample_bytree\\': 1.0,\\n    \\'lambda\\': 1.0,\\n    \\'tree_method\\': \\'hist\\',\\n    \\'seed\\': RANDOM_STATE,\\n}\\n\\nbst2 = xgb.train(\\n    params=params2,\\n    dtrain=dtrain2,\\n    num_boost_round=800,\\n    evals=[(dval2, \\'validation\\')],\\n    early_stopping_rounds=50,\\n    verbose_eval=False\\n)\\n\\nif hasattr(bst2, \\'best_ntree_limit\\') and bst2.best_ntree_limit:\\n    pred_te2 = np.expm1(bst2.predict(dtest2, ntree_limit=bst2.best_ntree_limit))\\nelif hasattr(bst2, \\'best_iteration\\') and bst2.best_iteration is not None:\\n    pred_te2 = np.expm1(bst2.predict(dtest2, iteration_range=(0, bst2.best_iteration + 1)))\\nelse:\\n    pred_te2 = np.expm1(bst2.predict(dtest2))\\n\\nmetrics2 = {\\n    \\'MAPE_eps1\\': mape_eps(Yte2, pred_te2, eps=1.0),\\n    \\'R2\\': r2(Yte2, pred_te2),\\n    \\'WAPE\\': wape(Yte2, pred_te2),\\n    \\'SMAPE\\': smape(Yte2, pred_te2),\\n    \\'MAE\\': mae(Yte2, pred_te2),\\n    \\'RMSE\\': rmse(Yte2, pred_te2)\\n}\\n\\nprint(\\'Model 2 (XGB, test) metrics:\\')\\nfor k_, v_ in metrics2.items():\\n    print(f\"- {k_}: {v_:.4f}\")\\n'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# =====================\n",
    "# (COMMENTED OUT) MODEL 2 TEMPLATE: wk2_weekly from wk1_total\n",
    "# Uncomment and adapt when ready to train second model.\n",
    "\"\"\"\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import train_test_split as tts\n",
    "import numpy as np\n",
    "import duckdb as ddb\n",
    "\n",
    "with ddb.connect(DB_PATH, read_only=True) as con:\n",
    "    df2 = con.execute('SELECT wk1_total, wk2_weekly FROM features_afterN1_week2').df()\n",
    "\n",
    "train_df2, test_df2 = train_test_split(df2, test_size=0.2, random_state=RANDOM_STATE)\n",
    "\n",
    "Xtr2 = np.log1p(train_df2[['wk1_total']].values.astype('float64'))\n",
    "Ytr2 = np.log1p(train_df2['wk2_weekly'].values.astype('float64'))\n",
    "Xte2 = np.log1p(test_df2[['wk1_total']].values.astype('float64'))\n",
    "Yte2 = test_df2['wk2_weekly'].values.astype('float64')\n",
    "\n",
    "Xtr2_sub, Xval2, Ytr2_sub, Yval2 = tts(Xtr2, Ytr2, test_size=0.2, random_state=RANDOM_STATE)\n",
    "\n",
    "dtrain2 = xgb.DMatrix(Xtr2_sub, label=Ytr2_sub)\n",
    "dval2 = xgb.DMatrix(Xval2, label=Yval2)\n",
    "dtest2 = xgb.DMatrix(Xte2)\n",
    "\n",
    "params2 = {\n",
    "    'objective': 'reg:squarederror',\n",
    "    'eval_metric': 'rmse',\n",
    "    'max_depth': 3,\n",
    "    'eta': 0.05,\n",
    "    'subsample': 0.9,\n",
    "    'colsample_bytree': 1.0,\n",
    "    'lambda': 1.0,\n",
    "    'tree_method': 'hist',\n",
    "    'seed': RANDOM_STATE,\n",
    "}\n",
    "\n",
    "bst2 = xgb.train(\n",
    "    params=params2,\n",
    "    dtrain=dtrain2,\n",
    "    num_boost_round=800,\n",
    "    evals=[(dval2, 'validation')],\n",
    "    early_stopping_rounds=50,\n",
    "    verbose_eval=False\n",
    ")\n",
    "\n",
    "if hasattr(bst2, 'best_ntree_limit') and bst2.best_ntree_limit:\n",
    "    pred_te2 = np.expm1(bst2.predict(dtest2, ntree_limit=bst2.best_ntree_limit))\n",
    "elif hasattr(bst2, 'best_iteration') and bst2.best_iteration is not None:\n",
    "    pred_te2 = np.expm1(bst2.predict(dtest2, iteration_range=(0, bst2.best_iteration + 1)))\n",
    "else:\n",
    "    pred_te2 = np.expm1(bst2.predict(dtest2))\n",
    "\n",
    "metrics2 = {\n",
    "    'MAPE_eps1': mape_eps(Yte2, pred_te2, eps=1.0),\n",
    "    'R2': r2(Yte2, pred_te2),\n",
    "    'WAPE': wape(Yte2, pred_te2),\n",
    "    'SMAPE': smape(Yte2, pred_te2),\n",
    "    'MAE': mae(Yte2, pred_te2),\n",
    "    'RMSE': rmse(Yte2, pred_te2)\n",
    "}\n",
    "\n",
    "print('Model 2 (XGB, test) metrics:')\n",
    "for k_, v_ in metrics2.items():\n",
    "    print(f\"- {k_}: {v_:.4f}\")\n",
    "\"\"\"\n",
    "# ====================="
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
