{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "128e733b",
   "metadata": {},
   "source": [
    "# Training Notebook (Model 1)\n",
    "\n",
    "Trains XGBoost model for final_total from wk1_total and saves artifacts to artifacts/final_total."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dd9b026",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 1469 rows from DuckDB.\n",
      "Data sample:\n",
      "     wk1_total  final_total\n",
      "0   2962981.0    3204876.0\n",
      "1  11268323.0   19806027.0\n",
      "2  22051768.0   40301616.0\n",
      "Model 1 (XGB, test) metrics:\n",
      "- MAPE_eps1: 0.3229\n",
      "- R2: 0.8204\n",
      "- WAPE: 0.3191\n",
      "- SMAPE: 0.3138\n",
      "- MAE: 47109513.7283\n",
      "- RMSE: 199400512.8425\n",
      "Artifacts saved to ../artifacts/final_total. Booster file exists: True size=127481 bytes\n"
     ]
    }
   ],
   "source": [
    "# 1. Train Model 1 (final_total from wk1_total)\n",
    "import os\n",
    "import math\n",
    "import sys\n",
    "import shutil\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "PARENT_DIR = os.path.abspath(os.path.join(os.getcwd(), '..'))\n",
    "if PARENT_DIR not in sys.path:\n",
    "    sys.path.insert(0, PARENT_DIR)\n",
    "\n",
    "RANDOM_STATE = 42\n",
    "EPS = 1.0\n",
    "DB_PATH = \"C:\\\\Users\\\\Nebula PC\\\\Swinburne\\\\Intern\\\\boxai\\\\data\\\\numero.duckdb\"\n",
    "TABLE_SQL = \"SELECT wk1_total, final_total FROM features_afterN1_total\"\n",
    "\n",
    "# ---- Metric ----\n",
    "def wape(y_true, y_pred):\n",
    "    denom = np.sum(np.abs(y_true)) + 1e-9\n",
    "    return float(np.sum(np.abs(y_true - y_pred) / denom)) if denom > 0 else np.nan\n",
    "\n",
    "def smape(y_true, y_pred):\n",
    "    denom = (np.abs(y_true) + np.abs(y_pred))\n",
    "    return float(np.mean(2.0 * np.abs(y_pred - y_true) / np.maximum(denom, 1e-9)))\n",
    "\n",
    "def mape_eps(y_true, y_pred, eps=EPS):\n",
    "    return float(np.mean(np.abs((y_true - y_pred) / np.maximum(np.abs(y_true), eps))))\n",
    "\n",
    "def mae(y_true, y_pred):\n",
    "    return float(np.mean(np.abs(y_true - y_pred)))\n",
    "\n",
    "def rmse(y_true, y_pred):\n",
    "    return float(np.sqrt(np.mean((y_true - y_pred) ** 2)))\n",
    "\n",
    "def r2(y_true, y_pred):\n",
    "    return float(r2_score(y_true, y_pred))\n",
    "\n",
    "# ---- Data Load  ----\n",
    "import duckdb as ddb\n",
    "with ddb.connect(DB_PATH, read_only=True) as con:\n",
    "    df = con.execute(TABLE_SQL).df()\n",
    "if df.empty:\n",
    "    raise ValueError('Query returned 0 rows.')\n",
    "print(f'Loaded {len(df)} rows from DuckDB.')\n",
    "print('Data sample:\\n', df.head(3))\n",
    "\n",
    "# ---- Split & Transform ----\n",
    "train_df, test_df = train_test_split(df, test_size=0.2, random_state=RANDOM_STATE)\n",
    "Xtr = np.log1p(train_df[['wk1_total']].values.astype('float64'))\n",
    "Ytr = np.log1p(train_df['final_total'].values.astype('float64'))\n",
    "Xte = np.log1p(test_df[['wk1_total']].values.astype('float64'))\n",
    "Yte = test_df['final_total'].values.astype('float64')\n",
    "\n",
    "Xtr_sub, Xval, Ytr_sub, Yval = train_test_split(Xtr, Ytr, test_size=0.2, random_state=RANDOM_STATE)\n",
    "\n",
    "dtrain = xgb.DMatrix(Xtr_sub, label=Ytr_sub)\n",
    "dval = xgb.DMatrix(Xval, label=Yval)\n",
    "dtest = xgb.DMatrix(Xte)\n",
    "\n",
    "params = {\n",
    "    'objective': 'reg:squarederror',\n",
    "    'eval_metric': 'rmse',\n",
    "    'max_depth': 3,\n",
    "    'eta': 0.05,\n",
    "    'subsample': 0.9,\n",
    "    'colsample_bytree': 1.0,\n",
    "    'lambda': 1.0,\n",
    "    'tree_method': 'hist',\n",
    "    'seed': RANDOM_STATE,\n",
    "}\n",
    "\n",
    "bst = xgb.train(\n",
    "    params=params,\n",
    "    dtrain=dtrain,\n",
    "    num_boost_round=800,\n",
    "    evals=[(dval, 'validation')],\n",
    "    early_stopping_rounds=50,\n",
    "    verbose_eval=False\n",
    ")\n",
    "\n",
    "if hasattr(bst, 'best_ntree_limit') and bst.best_ntree_limit:\n",
    "    pred_te = np.expm1(bst.predict(dtest, ntree_limit=bst.best_ntree_limit))\n",
    "elif hasattr(bst, 'best_iteration') and bst.best_iteration is not None:\n",
    "    pred_te = np.expm1(bst.predict(dtest, iteration_range=(0, bst.best_iteration + 1)))\n",
    "else:\n",
    "    pred_te = np.expm1(bst.predict(dtest))\n",
    "\n",
    "# ---- Metrics ----\n",
    "metrics = {\n",
    "    'MAPE_eps1': mape_eps(Yte, pred_te, eps=1.0),\n",
    "    'R2': r2(Yte, pred_te),\n",
    "    'WAPE': wape(Yte, pred_te),\n",
    "    'SMAPE': smape(Yte, pred_te),\n",
    "    'MAE': mae(Yte, pred_te),\n",
    "    'RMSE': rmse(Yte, pred_te)\n",
    "}\n",
    "print('Model 1 (XGB, test) metrics:')\n",
    "for k_, v_ in metrics.items():\n",
    "    print(f'- {k_}: {v_:.4f}')\n",
    "\n",
    "# ---- Save Artifacts ----\n",
    "from boxai.models.final_total_predictor import FinalTotalPredictor\n",
    "ART_DIR = '../artifacts/final_total'\n",
    "os.makedirs(ART_DIR, exist_ok=True)\n",
    "\n",
    "best_ntree_limit = getattr(bst, 'best_ntree_limit', None)\n",
    "best_iteration = getattr(bst, 'best_iteration', None)\n",
    "FinalTotalPredictor.save_from_training(\n",
    "    booster=bst,\n",
    "    artifacts_dir=ART_DIR,\n",
    "    best_ntree_limit=best_ntree_limit,\n",
    "    best_iteration=best_iteration,\n",
    "    metrics=metrics,\n",
    "    overwrite=True\n",
    ")\n",
    "model_path = os.path.join(ART_DIR, 'model.booster.json')\n",
    "print(f'Artifacts saved to {ART_DIR}. Booster file exists: {os.path.isfile(model_path)} size={os.path.getsize(model_path) if os.path.isfile(model_path) else 0} bytes')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92f7194a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (COMMENTED OUT) MODEL 2 TEMPLATE\n",
    "\"\"\"\n",
    "# =====================\n",
    "# (COMMENTED OUT) MODEL 2 TEMPLATE: wk2_weekly from wk1_total\n",
    "# Uncomment and adapt when ready to train second model.\n",
    "\"\"\"\n",
    "\"\"\"\"\"\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import train_test_split as tts\n",
    "import numpy as np\n",
    "import duckdb as ddb\n",
    "\n",
    "with ddb.connect(DB_PATH, read_only=True) as con:\n",
    "    df2 = con.execute('SELECT wk1_total, wk2_weekly FROM features_afterN1_week2').df()\n",
    "\n",
    "train_df2, test_df2 = train_test_split(df2, test_size=0.2, random_state=RANDOM_STATE)\n",
    "\n",
    "Xtr2 = np.log1p(train_df2[['wk1_total']].values.astype('float64'))\n",
    "Ytr2 = np.log1p(train_df2['wk2_weekly'].values.astype('float64'))\n",
    "Xte2 = np.log1p(test_df2[['wk1_total']].values.astype('float64'))\n",
    "Yte2 = test_df2['wk2_weekly'].values.astype('float64')\n",
    "\n",
    "Xtr2_sub, Xval2, Ytr2_sub, Yval2 = tts(Xtr2, Ytr2, test_size=0.2, random_state=RANDOM_STATE)\n",
    "\n",
    "dtrain2 = xgb.DMatrix(Xtr2_sub, label=Ytr2_sub)\n",
    "dval2 = xgb.DMatrix(Xval2, label=Yval2)\n",
    "dtest2 = xgb.DMatrix(Xte2)\n",
    "\n",
    "params2 = {\n",
    "    'objective': 'reg:squarederror',\n",
    "    'eval_metric': 'rmse',\n",
    "    'max_depth': 3,\n",
    "    'eta': 0.05,\n",
    "    'subsample': 0.9,\n",
    "    'colsample_bytree': 1.0,\n",
    "    'lambda': 1.0,\n",
    "    'tree_method': 'hist',\n",
    "    'seed': RANDOM_STATE,\n",
    "}\n",
    "\n",
    "bst2 = xgb.train(\n",
    "    params=params2,\n",
    "    dtrain=dtrain2,\n",
    "    num_boost_round=800,\n",
    "    evals=[(dval2, 'validation')],\n",
    "    early_stopping_rounds=50,\n",
    "    verbose_eval=False\n",
    ")\n",
    "\n",
    "if hasattr(bst2, 'best_ntree_limit') and bst2.best_ntree_limit:\n",
    "    pred_te2 = np.expm1(bst2.predict(dtest2, ntree_limit=bst2.best_ntree_limit))\n",
    "elif hasattr(bst2, 'best_iteration') and bst2.best_iteration is not None:\n",
    "    pred_te2 = np.expm1(bst2.predict(dtest2, iteration_range=(0, bst2.best_iteration + 1)))\n",
    "else:\n",
    "    pred_te2 = np.expm1(bst2.predict(dtest2))\n",
    "\n",
    "metrics2 = {\n",
    "    'MAPE_eps1': mape_eps(Yte2, pred_te2, eps=1.0),\n",
    "    'R2': r2(Yte2, pred_te2),\n",
    "    'WAPE': wape(Yte2, pred_te2),\n",
    "    'SMAPE': smape(Yte2, pred_te2),\n",
    "    'MAE': mae(Yte2, pred_te2),\n",
    "    'RMSE': rmse(Yte2, pred_te2)\n",
    "}\n",
    "\n",
    "print('Model 2 (XGB, test) metrics:')\n",
    "for k_, v_ in metrics2.items():\n",
    "    print(f\"- {k_}: {v_:.4f}\")\n",
    "\"\"\"\n",
    "# =====================\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
