{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QAjW6X8WR_wc",
        "outputId": "642ca984-ca1d-41db-a970-02048f1dcef2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== Running Grid Search for ExtraTreeRegressor (concurrent films model) ===\n",
            "Fitting 5 folds for each of 8 candidates, totalling 40 fits\n",
            "\n",
            "Best Parameters:\n",
            "max_depth: 10\n",
            "max_features: log2\n",
            "min_samples_split: 2\n",
            "\n",
            "Best MAPE: 0.0804\n",
            "\n",
            "ExtraTree - Fold 1 Metrics:\n",
            "MAPE: 243.64%\n",
            "RMSE: 3069941.80\n",
            "R2 Score: 0.6244\n",
            "\n",
            "ExtraTree - Fold 2 Metrics:\n",
            "MAPE: 194.00%\n",
            "RMSE: 4386932.53\n",
            "R2 Score: 0.5267\n",
            "\n",
            "ExtraTree - Fold 3 Metrics:\n",
            "MAPE: 219.89%\n",
            "RMSE: 2878166.19\n",
            "R2 Score: 0.6359\n",
            "\n",
            "ExtraTree - Fold 4 Metrics:\n",
            "MAPE: 177.55%\n",
            "RMSE: 3645615.34\n",
            "R2 Score: 0.5821\n",
            "\n",
            "ExtraTree - Fold 5 Metrics:\n",
            "MAPE: 207.16%\n",
            "RMSE: 3010962.02\n",
            "R2 Score: 0.6223\n"
          ]
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from sklearn.tree import ExtraTreeRegressor\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "import duckdb\n",
        "import pandas as pd\n",
        "import json\n",
        "import numpy as np\n",
        "from sklearn.model_selection import TimeSeriesSplit, GridSearchCV\n",
        "from sklearn.metrics import mean_absolute_percentage_error, mean_squared_error, r2_score, make_scorer\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "DB_PATH = 'numero.duckdb'\n",
        "\n",
        "def get_data(limit=100):\n",
        "    with duckdb.connect(DB_PATH) as con:\n",
        "        table_name = con.execute(\"SHOW TABLES\").fetchall()\n",
        "        df = con.execute(f\"SELECT * FROM {table_name[0][0]}\").df()\n",
        "    return df\n",
        "\n",
        "def JSON_to_DF(df_json):\n",
        "    df = df_json.copy()\n",
        "    df['films'] = df['data'].dropna().apply(lambda x: json.loads(x)['films'])\n",
        "    df = df.explode('films').reset_index(drop=True)\n",
        "    json_expanded = pd.json_normalize(df['films'])\n",
        "    json_expanded.columns = [f\"data.films.{col}\" for col in json_expanded.columns]\n",
        "    df = df.drop(columns=['data', 'films']).join(json_expanded)\n",
        "    df.columns = [col.replace(\"data.films.\", \"\") for col in df.columns]\n",
        "    return df\n",
        "\n",
        "def prepare_concurrent_features(df):\n",
        "    # Calculate first week gross for each film\n",
        "    df['first_week_gross'] = df['week.gross'].fillna(0) + df['weekend.gross'].fillna(0)\n",
        "\n",
        "    # Group by release date\n",
        "    features = []\n",
        "    for idx, row in df.iterrows():\n",
        "        date = row['releaseDate'] if 'releaseDate' in row else row.get('week_date', None)\n",
        "        censor = row['censorRating']\n",
        "        dist = row['distributorName']\n",
        "        # Get all other films released the same week (excluding this film)\n",
        "        concurrent = df[(df['releaseDate'] == date) & (df.index != idx)] if 'releaseDate' in df else df[(df['week_date'] == date) & (df.index != idx)]\n",
        "        concurrent_grosses = concurrent['first_week_gross'].values if len(concurrent) > 0 else [0]\n",
        "        features.append({\n",
        "            'week.theatreCount': row.get('week.theatreCount', 0),\n",
        "            'week.screenCount': row.get('week.screenCount', 0),\n",
        "            'weekend.theatreCount': row.get('weekend.theatreCount', 0),\n",
        "            'weekend.screenCount': row.get('weekend.screenCount', 0),\n",
        "            'concurrent_mean_gross': np.mean(concurrent_grosses),\n",
        "            'concurrent_median_gross': np.median(concurrent_grosses),\n",
        "            'censorRating': censor,\n",
        "            'distributorName': dist,\n",
        "            'first_week_gross': row['first_week_gross']\n",
        "        })\n",
        "    features_df = pd.DataFrame(features)\n",
        "    return features_df\n",
        "\n",
        "def prepare_features(df):\n",
        "    features_df = prepare_concurrent_features(df)\n",
        "    numeric_columns = ['week.theatreCount', 'week.screenCount',\n",
        "                       'weekend.theatreCount', 'weekend.screenCount',\n",
        "                       'concurrent_mean_gross', 'concurrent_median_gross'\n",
        "    ]\n",
        "    X = features_df[numeric_columns].copy()\n",
        "    scaler = StandardScaler()\n",
        "    X[numeric_columns] = scaler.fit_transform(X[numeric_columns])\n",
        "    le_censor = LabelEncoder()\n",
        "    le_dist = LabelEncoder()\n",
        "    X['censorRating_enc'] = le_censor.fit_transform(features_df['censorRating'])\n",
        "    X['distributorName_enc'] = le_dist.fit_transform(features_df['distributorName'])\n",
        "    y = np.log1p(features_df['first_week_gross'])\n",
        "    return X, y, scaler, le_censor, le_dist\n",
        "\n",
        "def evaluate_predictions(y_true, y_pred, model_name):\n",
        "    mape = mean_absolute_percentage_error(y_true, y_pred)\n",
        "    rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
        "    r2 = r2_score(y_true, y_pred)\n",
        "    print(f\"\\n{model_name} Metrics:\")\n",
        "    print(f\"MAPE: {mape:.2%}\")\n",
        "    print(f\"RMSE: {rmse:.2f}\")\n",
        "    print(f\"R2 Score: {r2:.4f}\")\n",
        "\n",
        "df_json = get_data()\n",
        "df = JSON_to_DF(df_json)\n",
        "# Calculate total gross for each movie\n",
        "df['total_gross'] = df['week.gross'].fillna(0) + df['weekend.gross'].fillna(0)\n",
        "\n",
        "threshold = df['total_gross'].quantile(0.95)\n",
        "df['category'] = df['total_gross'].apply(lambda x: 'Blockbuster' if x > threshold else 'Normal')\n",
        "df = df[df['category'] == 'Normal']\n",
        "\n",
        "X, y, scaler, le_censor, le_dist = prepare_features(df)\n",
        "tscv = TimeSeriesSplit(n_splits=5)\n",
        "param_grid = {\n",
        "    'max_depth': [10, 20],\n",
        "    'min_samples_split': [2, 5],\n",
        "    'max_features': ['sqrt', 'log2']\n",
        "}\n",
        "mape_scorer = make_scorer(mean_absolute_percentage_error, greater_is_better=False)\n",
        "print(\"\\n=== Running Grid Search for ExtraTreeRegressor (concurrent films model) ===\")\n",
        "grid_search = GridSearchCV(\n",
        "    estimator=ExtraTreeRegressor(),\n",
        "    param_grid=param_grid,\n",
        "    cv=tscv,\n",
        "    scoring=mape_scorer,\n",
        "    n_jobs=-1,\n",
        "    verbose=1\n",
        ")\n",
        "grid_search.fit(X, y)\n",
        "best_params = grid_search.best_params_\n",
        "best_score = grid_search.best_score_\n",
        "best_model = grid_search.best_estimator_\n",
        "print(f\"\\nBest Parameters:\")\n",
        "for param, value in best_params.items():\n",
        "    print(f\"{param}: {value}\")\n",
        "print(f\"\\nBest MAPE: {-best_score:.4f}\")\n",
        "# Evaluate on folds\n",
        "for i, (train_idx, test_idx) in enumerate(tscv.split(X)):\n",
        "    X_train, X_test = X.iloc[train_idx], X.iloc[test_idx]\n",
        "    y_train, y_test = y.iloc[train_idx], y.iloc[test_idx]\n",
        "    model = ExtraTreeRegressor(**best_params)\n",
        "    model.fit(X_train, y_train)\n",
        "    y_pred = model.predict(X_test)\n",
        "    y_test_orig = np.expm1(y_test)\n",
        "    y_pred_orig = np.expm1(y_pred)\n",
        "    evaluate_predictions(y_test_orig, y_pred_orig, f\"ExtraTree - Fold {i+1}\")\n",
        "# # Example prediction for a new film\n",
        "# new_film = {'censorRating': 'PG-13', 'distributorName': 'Universal Pictures'}\n",
        "# concurrent_films = [{'gross': 100000}, {'gross': 130000}]\n",
        "# # Aggregate concurrent features\n",
        "# count = len(concurrent_films)\n",
        "# total = sum(f['gross'] for f in concurrent_films)\n",
        "# mean = np.mean([f['gross'] for f in concurrent_films]) if concurrent_films else 0\n",
        "# median = np.median([f['gross'] for f in concurrent_films]) if concurrent_films else 0\n",
        "# max_g = max([f['gross'] for f in concurrent_films]) if concurrent_films else 0\n",
        "# X_new = np.array([[count, total, mean, median, max_g]])\n",
        "# # X_new = scaler.transform(X_new)\n",
        "# # censor_enc = le_censor.transform([new_film['censorRating']])[0] if new_film['censorRating'] in le_censor.classes_ else -1\n",
        "# # dist_enc = le_dist.transform([new_film['distributorName']])[0] if new_film['distributorName'] in le_dist.classes_ else -1\n",
        "# # X_pred = np.concatenate([X_new[0], [censor_enc, dist_enc]]).reshape(1, -1)\n",
        "# # y_pred = best_model.predict(X_pred)\n",
        "# # pred_gross = float(np.expm1(y_pred[0]))\n",
        "# # print(f\"Predicted first week gross for new film: ${pred_gross:,.2f}\")\n",
        "\n"
      ]
    }
  ]
}